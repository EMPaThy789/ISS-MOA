\chapter{Conclusion}
\label{chapter:Conclusion}

We have presented an iterative and feature drift adaptive algorithm for feature selection in a stream setting. Our results indicate that feature selection does indeed have benefits to classification and computation time in data streams. As established in chapter \ref{chapter:Results}, the proposed feature selection method shows improvement for kNN classification accuracy and computation time in data streams with irrelevant features. We also were able to obtain classification accuracies as good or better than kNN without feature selection for data streams without irrelevant features, indicating that the algorithm is unlikely to actively hinder classifier performance. Our method was also able to adapt to feature drift faster than many of the other commonly used classification algorithms in stream mining. As our proposed method is a relatively simple method specialised for the kNN classifier, the promising results obtained from our experiments lead us to believe that feature selection in stream mining has great viability and potential in addressing the constraints of the stream setting and is definitely an area of interest for further study.

Due to the constraints of time some ideas were not fully explored, implemented or tested in the project.

A limitation we faced in our investigation was the lack of real world stream datasets. While synthetic data can to an extent simulate real world problems, underlying relationships between features in real world data are often times more complex than that of synthesised data; therefore it would be of interest to see how the algorithm performs in real world data sets. Feature drift was only tested on data sets from one generator as we lacked other datasets and gradual feature drift was also not tested due to a lack of data. Further expansion on the generator to implement a gradual feature drift could be something to look into.

Using Naive Bayes as the classifier algorithm over kNN is something that can be explored. As NB makes independent Bayesian predictions for each attribute, adding new attributes to the classifier to make a new prediction is extremely cheap. This potentially allows the algorithm to cover all features which would remove the need for hill climbing and perhaps even $f$.

The kNN classifier implementation can potentially be improved as it is currently rather slow. The main issue of our implementation is the performance cost of finding the nearest neighbours which we do using quick select. Something we can potentially take advantage of is the fact that feature distances are normalised and the maximum distance added to the sum is $1.0$ per each additional feature. We could therefore potentially utilise this knowledge to create bounds on the subset's distance after certain iterations, potentially pruning certain subsets using this bound. This could potentially allow us to search through more subsets though the direct impact on the overall performance of the algorithm is unknown.

Different ranking functions can also be of interest as they directly affect performance. One metric which was not taken advantage of in our ranking functions was the difference in accuracy when a new feature is added to the subset. This difference can potentially be useful to ranking as in theory adding irrelevant features is unlikely to increase the prediction accuracy; meaning that potentially the ranking can identify these irrelevant features and adjust accordingly. A ranking function which takes advantage of this can potentially be interesting and should be explored.